{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "test_project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kemi41/orbit/blob/main/test_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvPdRNZBMH6a"
      },
      "source": [
        "### Creating JSON to Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNJP1s52AzgX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMvKLI3cmYwx",
        "outputId": "7cd4147f-bc29-41c2-eadf-4080fd5213c1"
      },
      "source": [
        "!wget -O plagcheckfile.csv https://raw.githubusercontent.com/arifanf/MSIB21/main/Dataset/plagcheckfile.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-08 01:59:16--  https://raw.githubusercontent.com/arifanf/MSIB21/main/Dataset/plagcheckfile.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1667 (1.6K) [text/plain]\n",
            "Saving to: ‘plagcheckfile.csv’\n",
            "\n",
            "\rplagcheckfile.csv     0%[                    ]       0  --.-KB/s               \rplagcheckfile.csv   100%[===================>]   1.63K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-08 01:59:16 (17.6 MB/s) - ‘plagcheckfile.csv’ saved [1667/1667]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPYkvf5zFGDL"
      },
      "source": [
        "PATH = os.path.dirname('/content/drive/MyDrive/Colab Notebooks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dilVm4XpElSH"
      },
      "source": [
        "# a function to clean the texts from the CSV file\n",
        "def cleanText(Text):\n",
        "    # Remove new lines within message\n",
        "    cleanedText = Text.replace('\\n',' ').lower()\n",
        "    # Deal with some weird tokens\n",
        "    cleanedText = cleanedText.replace(\"\\xc2\\xa0\", \"\")\n",
        "    # Remove punctuation\n",
        "    cleanedText = re.sub('([,])','', cleanedText)\n",
        "    # Remove multiple spaces in message\n",
        "    cleanedText = re.sub(' +',' ', cleanedText)\n",
        "    cleanedText = cleanedText.encode('ascii', 'ignore').decode('ascii')\n",
        "    return cleanedText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk5_bNsgE6Sc"
      },
      "source": [
        "# a function to get data from the CSV file\n",
        "def getData():\n",
        "    df = pd.read_csv('plagcheckfile.csv')\n",
        "    listTexts = df['Text'].values.tolist()\n",
        "    finallist=[]\n",
        "    print(listTexts)\n",
        "    for i in range (0,4):\n",
        "        textDictionary = {\"tag\":i}\n",
        "        listTexts_i = cleanText(listTexts[i])\n",
        "        print(listTexts_i)\n",
        "        textDictionary.update(texts = listTexts_i)\n",
        "        finallist.append(textDictionary)\n",
        "    finalDictionary={\"intents\":finallist}       \n",
        "    return finalDictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQY8Y21XFYaf",
        "outputId": "97ee6c8f-04f0-48dd-eae3-9b75566a6b23"
      },
      "source": [
        "# save the dictionary as a file\n",
        "combinedDictionary = dict()\n",
        "print ('Getting Text Data')\n",
        "combinedDictionary.update(getData())\n",
        "print ('Total len of dictionary', len(combinedDictionary))\n",
        "\n",
        "print ('Saving text data dictionary')\n",
        "np.save('\\\\textDictionary.npy', combinedDictionary)\n",
        "\n",
        "with open('\\\\file.txt', 'w') as file:\n",
        "     file.write(json.dumps(combinedDictionary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting Text Data\n",
            "['Summarizing a journal article is the process of presenting a focused overview of a completed research study that is published in a peer reviewed scholarly source. A journal article summary provides potential readers with a short descriptive commentary giving them some insight into the articles focus. Writing and summarizing a journal article is a common task for college students and research assistants alike. With a little practice you can learn to read the article effectively with an eye for summary plan a successful summary and write it to completion.', 'The first thing you should do is to decide why you need to summarize the article. If thepurpose of the summary is to take notes to later remind yourself about the article you maywant to write a longer summary. However if the purpose of summarizing the article is toinclude it in a paper you are writing the summary should focus on how the articlesrelates specifically to your paper.', 'An article summary is a short focused paper about one scholarly article that is informed by a critical reading of that article. For argumentative articles the summary identifies explains and analyses the thesis and supporting arguments for empirical articles the summary identifies explains and analyses the research questions methods findings and implications of the study.', 'Summarizing a journal article is the process of presenting to provides potential readers with a short descriptive commentary giving them some insight into the articles focus. Writing and summarizing a journal article is learn to read the article effectively with an eye for summary plan a successful summary and write it to completion.']\n",
            "summarizing a journal article is the process of presenting a focused overview of a completed research study that is published in a peer reviewed scholarly source. a journal article summary provides potential readers with a short descriptive commentary giving them some insight into the articles focus. writing and summarizing a journal article is a common task for college students and research assistants alike. with a little practice you can learn to read the article effectively with an eye for summary plan a successful summary and write it to completion.\n",
            "the first thing you should do is to decide why you need to summarize the article. if thepurpose of the summary is to take notes to later remind yourself about the article you maywant to write a longer summary. however if the purpose of summarizing the article is toinclude it in a paper you are writing the summary should focus on how the articlesrelates specifically to your paper.\n",
            "an article summary is a short focused paper about one scholarly article that is informed by a critical reading of that article. for argumentative articles the summary identifies explains and analyses the thesis and supporting arguments for empirical articles the summary identifies explains and analyses the research questions methods findings and implications of the study.\n",
            "summarizing a journal article is the process of presenting to provides potential readers with a short descriptive commentary giving them some insight into the articles focus. writing and summarizing a journal article is learn to read the article effectively with an eye for summary plan a successful summary and write it to completion.\n",
            "Total len of dictionary 1\n",
            "Saving text data dictionary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBzrabHjMDdL"
      },
      "source": [
        "### Creating the Machine Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOVThxaLKQGz",
        "outputId": "fcff54dd-c081-4e93-a522-007edaf7d3de"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "!pip install tflearn\n",
        "import tflearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 20 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 107 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=00c0a7be640dacc4d6d9fd9005702e0982eae998e9f87e29c2c14e1677459ff4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWm_z_VjKy77"
      },
      "source": [
        "stemmer = LancasterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR7PvsqUK2Yk"
      },
      "source": [
        "with open(\"\\\\file.txt\") as file:\n",
        "    dataset = json.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWlBQWBVLO8W"
      },
      "source": [
        "list_words = []\n",
        "labels = []\n",
        "docs_x = [] #List of all the question_patterns.\n",
        "docs_y = [] #List of all the tags for specific Texts."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbMv98emLUlP"
      },
      "source": [
        "### Stemming and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RAb9M3ML2mK",
        "outputId": "653b4f93-14e1-433f-b3fb-2e78d110de50"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZO4XsH7LSDk"
      },
      "source": [
        "for intent in dataset[\"intents\"]:\n",
        "    for pattern in intent[\"texts\"]: #Stems the words. Finds the root of the word. Removes extra characters and symbols to find the root word. \n",
        "        split_words = nltk.word_tokenize(pattern) #Tokenizes the words. Breakes the words in the places of spaces and returns a list of all the words in it.\n",
        "        list_words.extend(split_words) #Using instead of looping and adding one word at a time in the list. It just extends the list untill all the words are in it.\n",
        "        docs_x.append(split_words) #Adding the pattern of words inside docs_x list.\n",
        "        docs_y.append(intent[\"tag\"]) #For each pattern, it says what Tag it is a part of.\n",
        "\n",
        "    if intent[\"tag\"] not in labels:\n",
        "        labels.append(intent[\"tag\"]) #Adds all the tags in the labels list.\n",
        "\n",
        "list_words = [stemmer.stem(w.lower()) for w in list_words if w != \"?\"] #Lower cases all the words in the list_words list. \n",
        "list_words = sorted(list(set(list_words))) #Makes a set of the words to remove duplicate. This gives us the actual vocabulary size of the intent. Then converts it back to list and sorts it. \n",
        "\n",
        "labels = sorted(labels) #Sorts the labels where the tags are stored.\n",
        "\n",
        "training = [] #contains the bag of words.\n",
        "output = [] #The output list to choose the right tag for the output.\n",
        "\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "for x, doc in enumerate(docs_x):\n",
        "    bag = [] #Makes a list of all the words and tells if those words are occuring to find a pattern.\n",
        "    #We are representing each sentence with a list the length of the amount of words in our models vocabulary. \n",
        "    #Each position in the list will represent a word from our vocabulary. \n",
        "    #If the position in the list is a 1 then that will mean that the word exists in our sentence, if it is a 0 then the word is nor present. \n",
        "\n",
        "    split_words = [stemmer.stem(w.lower()) for w in doc] #Stems all the words inside docs_x list.\n",
        "\n",
        "    for w in list_words:\n",
        "        if w in split_words:\n",
        "            bag.append(1) #we are putting 1 in the bag of words list for the word (already present in the vocabulary list_words) present in the pattern and 0 if it is not.\n",
        "        else:\n",
        "            bag.append(0)\n",
        "\n",
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(docs_y[x])] = 1 #We are looking in the labels list to find were the tag is in that list. We are making that value 1 in the output row. \n",
        "    #We will create output lists which are the length of the amount of labels/tags we have in our dataset. \n",
        "    #Each position in the list will represent one distinct label/tag, a 1 in any of those positions will show which label/tag is represented.\n",
        "\n",
        "    training.append(bag) #We are putting the bag of words in the training list. \n",
        "    output.append(output_row) #We are putting the output_row list in the output list."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVmLVFp5L8KM"
      },
      "source": [
        "### Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3NHyBzoL5dr"
      },
      "source": [
        "def bag_of_words(s, list_words): #bag_of_words function will transform our string input to a bag of words using our created words list\n",
        "    bag = [0 for _ in range(len(list_words))]\n",
        "\n",
        "    inp_str_words = nltk.word_tokenize(s)\n",
        "    inp_str_words = [stemmer.stem(word.lower()) for word in inp_str_words]\n",
        "\n",
        "    for search_element in inp_str_words:\n",
        "        for i, w in enumerate(list_words):\n",
        "            if w == search_element:\n",
        "                bag[i] = 1 \n",
        "            \n",
        "    return numpy.array(bag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUgb42k3MQs_"
      },
      "source": [
        "### Building the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzWLDNMrMOab",
        "outputId": "2d8d1e9b-2ff0-4644-ed6a-c6c9a0778b8b"
      },
      "source": [
        "training = np.array(training) #we will convert our training data and output to numpy arrays.\n",
        "output = np.array(output)\n",
        "#tensorflow.reset_default_graph() #Resetting all the previous stuffs in the graph.\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])]) #This finds the input shape that we are expecting for the model. Each training input is gonna be of the same length, so, 0.\n",
        "net = tflearn.fully_connected(net, 8) #Hidden layer with 8 neurons.\n",
        "net = tflearn.fully_connected(net, 8) #Another hidden layer with 8 neurons.\n",
        "net = tflearn.fully_connected(net, 8) #Another hidden layer with 8 neurons.\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\") #length will be how many labels (how many tags) we have. Give us probability for each neuron in the network. The higest probability will be the output.\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net) #Model gets trained.\n",
        "\n",
        "try:\n",
        "    model.predict(PATH + \"\\\\model.tflearn\")\n",
        "    model.load()\n",
        "except:\n",
        "    model.fit(training, output, n_epoch=2000, batch_size=8, show_metric=True) #Fitting our data to the model. The number of epochs we set is the amount of times that the model will see the same information while training.\n",
        "    model.save(PATH + \"\\\\model.tflearn\") #we can save it to the file model.tflearn for use in other scripts."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 68938  | total loss: \u001b[1m\u001b[32m1.35090\u001b[0m\u001b[0m | time: 0.077s\n",
            "\u001b[2K\r| Adam | epoch: 334 | loss: 1.35090 - acc: 0.3662 -- iter: 0056/1650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1IFeuOIMmjj"
      },
      "source": [
        "### Function to Check Texts for Plagiarism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5fnZfnCMnxp"
      },
      "source": [
        "def check():\n",
        "     \n",
        "    print(\"Type an answer (Enter 'quit' to stop)\")\n",
        "    while True:\n",
        "        inp = input(\"You: \")\n",
        "        inp = inp.lower()\n",
        "        if inp == \"quit\":\n",
        "            break\n",
        "\n",
        "        results = model.predict([bag_of_words(inp, list_words)]) #we predict the most matching text from the training dataset that matches with the input text.\n",
        "        results_index = numpy.argmax(results) #we get thr index of the result.\n",
        "        tag = labels[results_index] #we get the tag of the most matching text with the input text.\n",
        "        \n",
        "        for tg in dataset[\"intents\"]:\n",
        "            if tg['tag'] == tag:\n",
        "                response = tg['texts']\n",
        "        print(\"Most similar text to the input: \")\n",
        "        print(response) #we show the most matching text with the input text. \n",
        "        split_sent_resp = nltk.sent_tokenize(response) #we divide the response text (the most matching text with the input text)\n",
        "        list_sents=split_sent_resp #we put the sentenses in a different list for future work.\n",
        "\n",
        "        split_sent_inp = nltk.sent_tokenize(inp) #we split the input text into sentenses\n",
        "        len_inp=len(split_sent_inp) #number of sentenses in the input text.\n",
        "        list_sents.extend(split_sent_inp)\n",
        "        total_len=len(list_sents) #we get the total number of sentenses in input and response text, including the duplicate sentenses.\n",
        "        set_sents=set(list_sents) #we are making the list a set, so that duplicate sentenses get deleted.\n",
        "        set_len=len(set_sents)\n",
        "        len_difference=total_len-set_len #this gives us the number of duplicate sentenses, i.e, the copied or plaigarized sentenses.\n",
        "        plag_percent=(1-((len_inp-len_difference)/len_inp))*100\n",
        "        print(\"The percent of plagarism in the document is: \", plag_percent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih878ueCMuip"
      },
      "source": [
        "### Checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV_hvmn-MrGf"
      },
      "source": [
        "check()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}